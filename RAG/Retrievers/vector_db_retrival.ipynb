{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28a72b7-7441-4223-9533-264ab0be9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/tst_imperial/langchain/venv/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50af3589-7e44-4566-94ef-bd27aa8f4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefc0809-f8b9-4d8d-8d6d-4eb21c1cda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872fb5ff-d07a-47bf-9ca8-ae10ef56fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = [\n",
    "        Document(page_content=\"\"\"Gradient Descent isn’t guaranteed to find the global minimum – \n",
    "        for non-convex functions (like deep neural networks), it often gets stuck in local minima or \n",
    "        saddle points, yet still works surprisingly well in practice.\"\"\"),\n",
    "        Document(page_content=\"\"\"Naïve Bayes can outperform complex models – despite its \"naïve\" assumption of feature \n",
    "        independence, it can work extremely well on text classification tasks like spam filtering.\"\"\"),\n",
    "        Document(page_content=\"\"\"PCA doesn’t just reduce dimensions—it denoises – by keeping only the top principal components,\n",
    "        PCA often filters out noise along with lowering dimensionality.\"\"\"),\n",
    "        Document(page_content=\"\"\"Support Vector Machines (SVMs) were once state-of-the-art – before deep learning dominated,\n",
    "        SVMs were the go-to algorithm for many real-world classification tasks.\"\"\"),\n",
    "        Document(page_content=\"\"\"Reinforcement Learning agents sometimes “cheat” – in simulations, RL agents have been known to \n",
    "        exploit bugs in the environment to maximize reward instead of solving the intended task.\"\"\")  \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c848807-17e7-4053-8790-144332ec7e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['e9fa657c-116b-44ea-b031-10247b67fd50',\n",
       "  '739cbd99-3249-4dbd-9fe4-ece37cbe182f',\n",
       "  '61fe76a1-573d-4625-95a7-4e9bf84a1787',\n",
       "  '1500bc55-3844-4dad-8b6a-6e6648523c63',\n",
       "  '73eb6ef2-9a0a-4f20-ba10-8d14d48ea87f'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['Gradient Descent isn’t guaranteed to find the global minimum – \\n        for non-convex functions (like deep neural networks), it often gets stuck in local minima or \\n        saddle points, yet still works surprisingly well in practice.',\n",
       "  'Naïve Bayes can outperform complex models – despite its \"naïve\" assumption of feature \\n        independence, it can work extremely well on text classification tasks like spam filtering.',\n",
       "  'PCA doesn’t just reduce dimensions—it denoises – by keeping only the top principal components,\\n        PCA often filters out noise along with lowering dimensionality.',\n",
       "  'Support Vector Machines (SVMs) were once state-of-the-art – before deep learning dominated,\\n        SVMs were the go-to algorithm for many real-world classification tasks.',\n",
       "  'Reinforcement Learning agents sometimes “cheat” – in simulations, RL agents have been known to \\n        exploit bugs in the environment to maximize reward instead of solving the intended task.'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [None, None, None, None, None]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_store=Chroma.from_documents(\n",
    "    documents=Docs,\n",
    "    embedding=embeddings\n",
    ")\n",
    "vec_store.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b61b1b-c94e-47e0-a284-55361b84e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vec_store.as_retriever(search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558c722c-5f44-4df9-b1b1-32f46630eb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Gradient Descent isn’t guaranteed to find the global minimum – \\n        for non-convex functions (like deep neural networks), it often gets stuck in local minima or \\n        saddle points, yet still works surprisingly well in practice.'),\n",
       " Document(metadata={}, page_content='Reinforcement Learning agents sometimes “cheat” – in simulations, RL agents have been known to \\n        exploit bugs in the environment to maximize reward instead of solving the intended task.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"whats the drawback of gradient desend?\"\n",
    "\n",
    "res = retriver.invoke(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d2c66e-3073-4466-beb8-3c66f4d94d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________\n",
      "Result: 1\n",
      "Gradient Descent isn’t guaranteed to find the global minimum – \n",
      "        for non-convex functions (like deep neural networks), it often gets stuck in local minima or \n",
      "        saddle points, yet still works surprisingly well in practice.\n",
      "_________________________________________________________________________________________\n",
      "_________________________________________________________________________________________\n",
      "Result: 2\n",
      "Reinforcement Learning agents sometimes “cheat” – in simulations, RL agents have been known to \n",
      "        exploit bugs in the environment to maximize reward instead of solving the intended task.\n",
      "_________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(res):\n",
    "    print(\"_________________________________________________________________________________________\")\n",
    "    print(\"Result:\",i+1)\n",
    "    print(doc.page_content)\n",
    "    print(\"_________________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad8710-053e-4f0c-8acf-2c63dbb1c632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
